{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from ridgeplot import ridgeplot\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gluformer.attention import *\n",
    "from gluformer.encoder import *\n",
    "from gluformer.decoder import *\n",
    "from gluformer.embed import *\n",
    "from gluformer.model import *\n",
    "from utils.train import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define save path\n",
    "save_path = './trials/trial_exp'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# define cache path\n",
    "cache_path = 'cache/visualize_experiment'\n",
    "if not os.path.exists(cache_path):\n",
    "    os.makedirs(cache_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mean with multi-modal behavior\n",
    "random.seed(0)\n",
    "def generate_mean(off, modes, lens, betas):\n",
    "    mean = np.zeros(sum(lens))\n",
    "    mean[:lens[0]] = off + \\\n",
    "                    np.sin((2*np.pi / 3) * np.arange(0, lens[0], 1)) * betas[0, 0] + \\\n",
    "                    np.cos((2*np.pi / 7) * np.arange(0, lens[0], 1)) * betas[0, 1]\n",
    "    state = random.randint(0, 1)\n",
    "    mean[lens[0]:sum(lens)] = off + modes[state] + \\\n",
    "                            np.sin((2*np.pi / 3) * np.arange(lens[0], sum(lens), 1)) * betas[1+state, 0] + \\\n",
    "                            np.cos((2*np.pi / 7) * np.arange(lens[0], sum(lens), 1)) * betas[1+state, 1]\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the covariance matrix and sample from the Gaussian distribution \\\n",
    "# with specified mean and covariance\n",
    "lens = [5, 9]\n",
    "modes = [-10, 10]\n",
    "off = 0\n",
    "betas = np.array([[0.5, 0.5], [0.3, 0.7], [0.7, 0.3]])\n",
    "\n",
    "cov = np.zeros((sum(lens), sum(lens)))\n",
    "row = np.array([1] + [1 / abs(i) for i in range(1, sum(lens))])\n",
    "for i in range(sum(lens)):\n",
    "    cov[i, :(i+1)] = np.flip(row[:(i+1)])\n",
    "    cov[i, (i+1):] = row[1:(sum(lens)-i)]\n",
    "np.fill_diagonal(cov, 2)\n",
    "\n",
    "train_samples = 2000\n",
    "val_samples = 100\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "for i in range(train_samples):\n",
    "    if i < val_samples:\n",
    "        val_data.append([0,\n",
    "                        np.random.multivariate_normal(generate_mean(off, modes, lens, betas), cov).reshape((-1, 1)), \n",
    "                        np.arange(sum(lens), dtype=np.float32).reshape((-1, 1))])\n",
    "        test_data.append([0,\n",
    "                        np.random.multivariate_normal(generate_mean(off, modes, lens, betas), cov).reshape((-1, 1)), \n",
    "                        np.arange(sum(lens), dtype=np.float32).reshape((-1, 1))])\n",
    "\n",
    "    train_data.append([0,\n",
    "                    np.random.multivariate_normal(generate_mean(off, modes, lens, betas), cov).reshape((-1, 1)), \n",
    "                    np.arange(sum(lens), dtype=np.float32).reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data to [0, 1] using the min and max of the training data\n",
    "maxx = np.max([train_data[i][1].max() for i in range(len(train_data))])\n",
    "minn = np.min([train_data[i][1].min() for i in range(len(train_data))])\n",
    "\n",
    "train_data = [[data[0],\n",
    "              (data[1] - minn) / (maxx - minn),\n",
    "              data[2] / sum(lens)] for data in train_data]\n",
    "\n",
    "val_data = [[data[0],\n",
    "              (data[1] - minn) / (maxx - minn),\n",
    "              data[2] / sum(lens)] for data in val_data]\n",
    "\n",
    "\n",
    "test_data = [[data[0],\n",
    "              (data[1] - minn) / (maxx - minn),\n",
    "              data[2] / sum(lens)] for data in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test samples\n",
    "plot_test = np.vstack([data[1].reshape((1, -1)) for data in test_data])\n",
    "out = plt.plot(plot_test.transpose())\n",
    "plt.show()\n",
    "# save full data\n",
    "np.save(f\"{cache_path}/test_data.npy\", plot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  def __init__(self, data, flag='train', size=None):\n",
    "    # size [seq_len, label_len, pred_len]\n",
    "    # info\n",
    "    self.seq_len = size[0]\n",
    "    self.label_len = size[1]\n",
    "    self.pred_len = size[2]\n",
    "    \n",
    "    # init\n",
    "    self.data = data\n",
    "    len_segs = np.array([len(subj_seg[1]) for subj_seg in self.data])\n",
    "    len_segs = len_segs - self.seq_len - self.pred_len + 1\n",
    "    self.len_segs = np.insert(np.cumsum(len_segs), 0, 0)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    idx_seg = np.argmax(self.len_segs > index) - 1\n",
    "    seg = self.data[idx_seg]\n",
    "\n",
    "    s_begin = index - self.len_segs[idx_seg]\n",
    "    s_end = s_begin + self.seq_len\n",
    "    r_begin = s_end - self.label_len\n",
    "    r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "    subj_id = seg[0]\n",
    "    seq_x = seg[1][s_begin:s_end]\n",
    "    seq_y = seg[1][r_begin:r_end]\n",
    "    seq_x_mark = seg[2][s_begin:s_end]\n",
    "    seq_y_mark = seg[2][r_begin:r_end]\n",
    "\n",
    "    return subj_id, seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.len_segs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq, len_label, len_pred = 4, 2, 2\n",
    "\n",
    "train_data = Data(train_data, size=[len_seq, len_label, len_pred])\n",
    "val_data = Data(val_data, size=[len_seq, len_label, len_pred])\n",
    "test_data = Data(test_data, size=[len_seq, len_label, len_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = Gluformer(d_model=256, \n",
    "                    n_heads=4, \n",
    "                    d_fcn=1024, \n",
    "                    r_drop=0.3, \n",
    "                    activ=\"relu\", \n",
    "                    num_enc_layers=2, \n",
    "                    num_dec_layers=1,\n",
    "                    distil=True,\n",
    "                    len_seq=len_seq, \n",
    "                    len_pred=len_pred,\n",
    "                    num_features=1)\n",
    "model.train()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the infinite mixture objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(save_path, \"model_inf_mixture.pth\")\n",
    "num_samples = 10 # number of samples for MC estimate\n",
    "batch_size = 256 # batch size for optimization\n",
    "collate_fn_custom = modify_collate(num_samples)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True, \n",
    "                                collate_fn = collate_fn_custom)\n",
    "\n",
    "val_data_loader = DataLoader(val_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True, \n",
    "                                collate_fn = collate_fn_custom)\n",
    "\n",
    "test_data_loader = DataLoader(test_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True,\n",
    "                                collate_fn = collate_fn_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "criterion =  ExpLikeliLoss(num_samples)\n",
    "model_optim = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0, 0.9))\n",
    "\n",
    "# define params for training\n",
    "TRAIN_STEPS = len(train_data_loader)\n",
    "early_stop = EarlyStop(100, 1e-6)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "    \n",
    "    epoch_time = time.time()\n",
    "    curr_time = time.time()\n",
    "    \n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_data_loader):\n",
    "        iter_count += 1\n",
    "        # zero-out grad\n",
    "        model_optim.zero_grad()\n",
    "        pred, true, logvar = process_batch(subj_id=subj_id,\n",
    "                                            batch_x=batch_x, \n",
    "                                            batch_y=batch_y, \n",
    "                                            batch_x_mark=batch_x_mark, \n",
    "                                            batch_y_mark=batch_y_mark, \n",
    "                                            len_pred=len_pred, \n",
    "                                            len_label=len_label, \n",
    "                                            model=model, \n",
    "                                            device=device)\n",
    "        loss = criterion(pred, true, logvar)\n",
    "        train_loss.append(float(loss.item()))\n",
    "        \n",
    "        # print every 10\n",
    "        if (i+1) % 10==0:\n",
    "            print(\"\\t iters: {0} / {3}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item(), TRAIN_STEPS))\n",
    "            logvar = logvar.reshape(-1, num_samples)\n",
    "            print(f\"\\t variance shape: {logvar.shape}\")\n",
    "            print(\"\\t variance: \", np.exp(logvar.detach().cpu().numpy()[0, :]))\n",
    "            speed = (time.time() - curr_time) / iter_count\n",
    "            left_time = speed * ((epochs - epoch) * TRAIN_STEPS - i)\n",
    "            print('\\t speed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            curr_time = time.time()\n",
    "        \n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "    # compute average train loss\n",
    "    train_loss = np.average(train_loss)\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(val_data_loader):\n",
    "            pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                    batch_x=batch_x, \n",
    "                                    batch_y=batch_y, \n",
    "                                    batch_x_mark=batch_x_mark, \n",
    "                                    batch_y_mark=batch_y_mark, \n",
    "                                    len_pred=len_pred, \n",
    "                                    len_label=len_label, \n",
    "                                    model=model, \n",
    "                                    device=device)\n",
    "            loss = criterion(pred, true, logvar)\n",
    "            val_loss.append(float(loss.item()))\n",
    "        val_loss = np.average(val_loss)\n",
    "    \n",
    "    # check early stopping\n",
    "    early_stop(val_loss, model, model_path)\n",
    "    if early_stop.stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    # update lr\n",
    "    # adjust_learning_rate(model_optim, epoch, lr)\n",
    "    \n",
    "    print(\"Epoch: {0} Time: {1} Steps: {2}\".format(epoch+1, time.time() - epoch_time, TRAIN_STEPS))\n",
    "    print(\"Train Loss: {0:.7f} | Val Loss: {1:.7f}\".format(train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test infinite mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "calibration = [[] for i in range(len_pred)]\n",
    "ape, rmse = [], []\n",
    "with torch.no_grad():\n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_data_loader):\n",
    "        pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                batch_x=batch_x, \n",
    "                                batch_y=batch_y, \n",
    "                                batch_x_mark=batch_x_mark, \n",
    "                                batch_y_mark=batch_y_mark, \n",
    "                                len_pred=len_pred, \n",
    "                                len_label=len_label, \n",
    "                                model=model, \n",
    "                                device=device)\n",
    "        \n",
    "        # arrange in proper shape: take mean of predicted samples\n",
    "        pred = pred.detach().cpu().numpy(); true = true.detach().cpu().numpy(); logvar = logvar.detach().cpu().numpy()\n",
    "        pred = pred.transpose((1,0,2)).reshape((pred.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        true = true.transpose((1,0,2)).reshape((true.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        logvar = logvar.transpose((1,0,2)).reshape((logvar.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        \n",
    "        # calculate calibration\n",
    "        for k in range(batch_size):\n",
    "            for i in range(len_pred):\n",
    "                ps = [norm.cdf(true[k, i, 0], pred[k, i, j], np.sqrt(np.exp(logvar[k, 0, j]))) \n",
    "                        for j in range(num_samples)]\n",
    "                p = np.average(ps)\n",
    "                calibration[i].append(p)\n",
    "\n",
    "        # for metrics: take mean of smaples, take one sample of true\n",
    "        pred = np.mean(pred, axis=2)\n",
    "        true = true[:, :, 0]\n",
    "        # compute APE / RMSE\n",
    "        ape.append(np.mean(np.abs(true - pred) / true))\n",
    "        rmse.append(np.sqrt(np.mean((true - pred)**2)))\n",
    " \n",
    "rmse = np.median(rmse)\n",
    "ape = np.median(ape)\n",
    "print(\"APE: {0:.7f}\".format(ape))\n",
    "print(\"RMSE: {0:.7f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for plotting\n",
    "num_samples=5 # increase number of samples from posterior\n",
    "collate_fn_custom = modify_collate(num_samples)\n",
    "test_data_loader = DataLoader(test_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True,\n",
    "                                collate_fn = collate_fn_custom)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_data_loader):\n",
    "        pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                batch_x=batch_x, \n",
    "                                batch_y=batch_y, \n",
    "                                batch_x_mark=batch_x_mark, \n",
    "                                batch_y_mark=batch_y_mark, \n",
    "                                len_pred=len_pred, \n",
    "                                len_label=len_label, \n",
    "                                model=model, \n",
    "                                device=device)\n",
    "        \n",
    "        # arrange in proper shape: take mean of predicted samples\n",
    "        pred = pred.detach().cpu().numpy(); true = true.detach().cpu().numpy()\n",
    "        batch_x = batch_x.detach().cpu().numpy(); logvar = logvar.detach().cpu().numpy()\n",
    "        batch_x_mark = batch_x_mark.detach().cpu().numpy()\n",
    "        pred = pred.transpose((1,0,2)).reshape((pred.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        true = true.transpose((1,0,2)).reshape((true.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        logvar = logvar.transpose((1,0,2)).reshape((logvar.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        batch_x = batch_x.transpose((1,0,2)).reshape((batch_x.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        batch_x_mark = batch_x_mark.transpose((1,0,2)).reshape((batch_x_mark.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        break\n",
    "# save\n",
    "np.save(f\"{cache_path}/pred_mean_infmixt.npy\", pred)\n",
    "np.save(f\"{cache_path}/true_mean_infmixt.npy\", true)\n",
    "np.save(f\"{cache_path}/pred_logvar_infmixt.npy\", logvar)\n",
    "np.save(f\"{cache_path}/input_infmixt.npy\", batch_x)\n",
    "np.save(f\"{cache_path}/input_x_norm.npy\", batch_x_mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = Gluformer(d_model=128, \n",
    "                    n_heads=2, \n",
    "                    d_fcn=128, \n",
    "                    r_drop=0., \n",
    "                    activ=\"relu\", \n",
    "                    num_enc_layers=2, \n",
    "                    num_dec_layers=1,\n",
    "                    distil=True,\n",
    "                    len_seq=len_seq, \n",
    "                    len_pred=len_pred,\n",
    "                    num_features=1)\n",
    "model.train()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(save_path, \"model_norm.pth\")\n",
    "num_samples = 1 \n",
    "batch_size = 256 # batch size for optimization\n",
    "collate_fn_custom = modify_collate(num_samples)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True, \n",
    "                                collate_fn = collate_fn_custom)\n",
    "\n",
    "val_data_loader = DataLoader(val_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True, \n",
    "                                collate_fn = collate_fn_custom)\n",
    "\n",
    "test_data_loader = DataLoader(test_data, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False, \n",
    "                                num_workers=0, \n",
    "                                drop_last=True,\n",
    "                                collate_fn = collate_fn_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "model_optim = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0, 0.9))\n",
    "\n",
    "# define params for training\n",
    "TRAIN_STEPS = len(train_data_loader)\n",
    "early_stop = EarlyStop(5, 1e-6)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "    \n",
    "    epoch_time = time.time()\n",
    "    curr_time = time.time()\n",
    "    \n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_data_loader):\n",
    "        iter_count += 1\n",
    "        # zero-out grad\n",
    "        model_optim.zero_grad()\n",
    "        pred, true, logvar = process_batch(subj_id=subj_id,\n",
    "                                            batch_x=batch_x, \n",
    "                                            batch_y=batch_y, \n",
    "                                            batch_x_mark=batch_x_mark, \n",
    "                                            batch_y_mark=batch_y_mark, \n",
    "                                            len_pred=len_pred, \n",
    "                                            len_label=len_label, \n",
    "                                            model=model, \n",
    "                                            device=device)\n",
    "        loss = criterion(pred, true)\n",
    "        train_loss.append(float(loss.item()))\n",
    "        \n",
    "        # print every 10\n",
    "        if (i+1) % 10==0:\n",
    "            print(\"\\t iters: {0} / {3}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item(), TRAIN_STEPS))\n",
    "            speed = (time.time() - curr_time) / iter_count\n",
    "            left_time = speed * ((epochs - epoch) * TRAIN_STEPS - i)\n",
    "            print('\\t speed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            curr_time = time.time()\n",
    "        \n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "    # compute average train loss\n",
    "    train_loss = np.average(train_loss)\n",
    "\n",
    "    # compute validation / test loss + test metric\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(val_data_loader):\n",
    "            pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                    batch_x=batch_x, \n",
    "                                    batch_y=batch_y, \n",
    "                                    batch_x_mark=batch_x_mark, \n",
    "                                    batch_y_mark=batch_y_mark, \n",
    "                                    len_pred=len_pred, \n",
    "                                    len_label=len_label, \n",
    "                                    model=model, \n",
    "                                    device=device)\n",
    "            loss = criterion(pred, true)\n",
    "            val_loss.append(float(loss.item()))\n",
    "        val_loss = np.average(val_loss)\n",
    "    \n",
    "    # check early stopping\n",
    "    early_stop(val_loss, model, model_path)\n",
    "    if early_stop.stop:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    # update lr\n",
    "    # adjust_learning_rate(model_optim, epoch, lr)\n",
    "    \n",
    "    print(\"Epoch: {0} Time: {1} Steps: {2}\".format(epoch+1, time.time() - epoch_time, TRAIN_STEPS))\n",
    "    print(\"Train Loss: {0:.7f} | Val Loss: {1:.7f}\".format(train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ape, rmse, likelihood = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_data_loader):\n",
    "        pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                batch_x=batch_x, \n",
    "                                batch_y=batch_y, \n",
    "                                batch_x_mark=batch_x_mark, \n",
    "                                batch_y_mark=batch_y_mark, \n",
    "                                len_pred=len_pred, \n",
    "                                len_label=len_label, \n",
    "                                model=model, \n",
    "                                device=device)\n",
    "        \n",
    "        # arrange in proper shape: take mean of predicted samples\n",
    "        pred = pred.detach().cpu().numpy(); true = true.detach().cpu().numpy(); logvar = logvar.detach().cpu().numpy()\n",
    "        pred = pred.transpose((1,0,2)).reshape((pred.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        true = true.transpose((1,0,2)).reshape((true.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        # for metrics: take mean of smaples, take one sample of true\n",
    "        pred = np.mean(pred, axis=2)\n",
    "        true = true[:, :, 0]\n",
    "        # compute APE / RMSE\n",
    "        ape.append(np.mean(np.abs(true - pred) / true))\n",
    "        rmse.append(np.sqrt(np.mean((true - pred)**2)))\n",
    "        # compute likelihood for computing var later\n",
    "        likelihood.append(np.mean((pred - true)**2, axis=1))\n",
    "\n",
    "varhat = np.mean(np.concatenate(likelihood, axis=0))\n",
    "rmse = np.median(rmse)\n",
    "ape = np.median(ape)\n",
    "print(\"APE: {0:.7f}\".format(ape))\n",
    "print(\"RMSE: {0:.7f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for plotting\n",
    "with torch.no_grad():\n",
    "    for i, (subj_id, batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_data_loader):\n",
    "        pred, true, logvar = process_batch(subj_id = subj_id, \n",
    "                                batch_x=batch_x, \n",
    "                                batch_y=batch_y, \n",
    "                                batch_x_mark=batch_x_mark, \n",
    "                                batch_y_mark=batch_y_mark, \n",
    "                                len_pred=len_pred, \n",
    "                                len_label=len_label, \n",
    "                                model=model, \n",
    "                                device=device)\n",
    "        \n",
    "        # arrange in proper shape: take mean of predicted samples\n",
    "        pred = pred.detach().cpu().numpy(); true = true.detach().cpu().numpy()\n",
    "        batch_x = batch_x.detach().cpu().numpy(); batch_x_mark = batch_x_mark.detach().cpu().numpy()\n",
    "        pred = pred.transpose((1,0,2)).reshape((pred.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        true = true.transpose((1,0,2)).reshape((true.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        batch_x = batch_x.transpose((1,0,2)).reshape((batch_x.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        batch_x_mark = batch_x_mark.transpose((1,0,2)).reshape((batch_x_mark.shape[1], -1, num_samples)).transpose((1, 0, 2))\n",
    "        break\n",
    "# save\n",
    "np.save(f\"{cache_path}/pred_mean_norm.npy\", pred)\n",
    "np.save(f\"{cache_path}/true_mean_norm.npy\", true)\n",
    "np.save(f\"{cache_path}/pred_var_norm.npy\", np.array([varhat]))\n",
    "np.save(f\"{cache_path}/input_norm.npy\", batch_x)\n",
    "np.save(f\"{cache_path}/input_x_norm.npy\", batch_x_mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285382b90770c3e479aa722115be92e757ae824c82ed0278c4e38e83bd68deda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gluformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
